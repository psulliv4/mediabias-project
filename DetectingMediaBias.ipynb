{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be4nBharassa"
   },
   "source": [
    "The first step in our process was to scrape the news articles.  We decided to use News API, because it covered a large range of sources and was free to use for our purposes.  One drawback was that it only allowed us to scrape articles in the past 30 days.  \n",
    "\n",
    "We used the Media Bias chart created by Ad Fontes Media to determine which sources to pull from.  Our initial scrape we decided to use 1 conservative and 1 liberal source that were described as hyper partisan by the chart and 2 sources that were described as skewed.  We then scraped 3 sources that were described as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBZB8O_abuYb"
   },
   "source": [
    "News API requires you to use a keyword (denoted by 'q = ') to scrape articles, it will scrape the articles that have that keyword located within them.  For our initial scrape we decided to use broad terms to see how many articles we can get.  Further scrapes we may need to be more precise about the terms we use, but need to consider how that may bias our results.\n",
    "\n",
    "The pageSize attribute is how many articles from the batch that will be included in the list object that is retrieved.  For our initial scrape we decided to use 100.  In future scrapes we may decide to use a higher number, or lower.\n",
    "\n",
    "Sorting by popularity will allow us to retrieve the retrieve the most viewed articles of the bunch.  We may also need to consider how this could potentially bias our results.\n",
    "\n",
    "This process was repeated almost identically for the Conservative, Liberal, and Neutral news sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPK106ZH832q"
   },
   "source": [
    "# Conservative Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WVE-maUmXq2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url_fox1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=fox-news&'\n",
    "       'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_fox1 = requests.get(url_fox1)\n",
    "\n",
    "#print(response_fox1.content)\n",
    "\n",
    "url_fox2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=fox-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_fox2 = requests.get(url_fox2)\n",
    "\n",
    "#response_fox2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQOA-qUG6Zn2"
   },
   "outputs": [],
   "source": [
    "url_nreview1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview1 = requests.get(url_nreview1)\n",
    "\n",
    "#print(response_nreview1.content)\n",
    "\n",
    "url_nreview2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview2 = requests.get(url_nreview2)\n",
    "\n",
    "#print(response_nreview2.content)\n",
    "\n",
    "url_nreview3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=national-review&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_nreview3 = requests.get(url_nreview3)\n",
    "\n",
    "#response_nreview3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGgmVNLT7Pk2"
   },
   "outputs": [],
   "source": [
    "url_bart1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=breitbart-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_bart1 = requests.get(url_bart1)\n",
    "\n",
    "#print(response_bart1.content)\n",
    "\n",
    "url_bart2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=breitbart-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_bart2 = requests.get(url_bart2)\n",
    "\n",
    "#response_bart2.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TuNe--_8yea"
   },
   "source": [
    "# Liberal Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV-NlflP8xAt"
   },
   "outputs": [],
   "source": [
    "url_vice1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=vice-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_vice1 = requests.get(url_vice1)\n",
    "\n",
    "#print(response_vice1.content)\n",
    "\n",
    "url_vice2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=vice-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_vice2 = requests.get(url_vice2)\n",
    "\n",
    "#print(response_vice2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SobKN6njAU01"
   },
   "outputs": [],
   "source": [
    "url_msnbc1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc1 = requests.get(url_msnbc1)\n",
    "\n",
    "#print(response_msnbc1.content)\n",
    "\n",
    "url_msnbc2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=what&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc2 = requests.get(url_msnbc2)\n",
    "\n",
    "#print(response_msnbc2.content)\n",
    "\n",
    "url_msnbc3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=msnbc&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_msnbc3 = requests.get(url_msnbc3)\n",
    "\n",
    "#print(response_msnbc3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-kC7Wi1BcZO"
   },
   "outputs": [],
   "source": [
    "url_buzz1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz1 = requests.get(url_buzz1)\n",
    "\n",
    "#print(response_buzz1.content)\n",
    "\n",
    "url_buzz2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz2 = requests.get(url_buzz2)\n",
    "\n",
    "#print(response_buzz2.content)\n",
    "\n",
    "url_buzz3 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=news&'\n",
    "       'sources=buzzfeed&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_buzz3 = requests.get(url_buzz3)\n",
    "\n",
    "#print(response_buzz3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8l1twfXjCGPb"
   },
   "source": [
    "# Neutral Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6A2Yz537CFSE"
   },
   "outputs": [],
   "source": [
    "url_cbs1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=cbs-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_cbs1 = requests.get(url_cbs1)\n",
    "\n",
    "#print(response_cbs1.content)\n",
    "\n",
    "url_cbs2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=cbs-news&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_cbs2 = requests.get(url_cbs2)\n",
    "\n",
    "#print(response_cbs2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDtcr4HpLPgv"
   },
   "outputs": [],
   "source": [
    "url_hill1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=the-hill&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_hill1 = requests.get(url_hill1)\n",
    "\n",
    "#print(response_hill1.content)\n",
    "\n",
    "url_hill2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=virus&'\n",
    "       'sources=the-hill&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_hill2 = requests.get(url_hill2)\n",
    "\n",
    "#print(response_hill2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_lcHrwmLoGE"
   },
   "outputs": [],
   "source": [
    "url_reuters1 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=primary&'\n",
    "       'sources=reuters&'\n",
    "        'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-02-24&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_reuters1 = requests.get(url_reuters1)\n",
    "\n",
    "#print(response_reuters1.content)\n",
    "\n",
    "url_reuters2 = ('http://newsapi.org/v2/everything?'\n",
    "       'q=trump&'\n",
    "       'sources=reuters&'\n",
    "       'pageSize=100&'\n",
    "       'sortBy=popularity&'\n",
    "       'from=2020-03-15&'\n",
    "       'to=2020-03-24&'\n",
    "       'apiKey=dfb5e6075cc043b687b45876d6be0561')\n",
    "\n",
    "response_reuters2 = requests.get(url_reuters2)\n",
    "\n",
    "#print(response_reuters2.content)\n",
    "# response_reuters2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7T04boIMGx_"
   },
   "source": [
    "# Decoding and Converting to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOxXM5MheLUD"
   },
   "source": [
    "Using the decode function was necessary to translate the list objects from News API so they could be converted to a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd5YejjpvSXR"
   },
   "outputs": [],
   "source": [
    "# response_fox1\n",
    "# response_fox2\n",
    "# response_bart1\n",
    "# response_bart2\n",
    "# response_nreview1\n",
    "# response_nreview2\n",
    "# response_nreview3\n",
    "\n",
    "print('Conservative sources, 1 partisan right, 2 skew right')\n",
    "response_fox1_d = response_fox1.content.decode(\"utf-8\")\n",
    "print(response_fox1_d)\n",
    "response_fox2_d = response_fox2.content.decode(\"utf-8\")\n",
    "print(response_fox2_d)\n",
    "\n",
    "response_bart1_d = response_bart1.content.decode(\"utf-8\")\n",
    "#print(response_bart1_d)\n",
    "response_bart2_d = response_bart2.content.decode(\"utf-8\")\n",
    "#print(response_bart2_d)\n",
    "\n",
    "response_nreview1_d = response_nreview1.content.decode(\"utf-8\")\n",
    "#print(response_nreview1_d)\n",
    "response_nreview2_d = response_nreview2.content.decode(\"utf-8\")\n",
    "#print(response_nreview2_d)\n",
    "response_nreview3_d = response_nreview3.content.decode(\"utf-8\")\n",
    "#print(response_nreview3_d)\n",
    "\n",
    "\n",
    "# response_vice1\n",
    "# response_vice2\n",
    "# response_msnbc1\n",
    "# response_msnbc2\n",
    "# response_msnbc3\n",
    "# response_buzz1\n",
    "# response_buzz2\n",
    "# response_buzz3\n",
    "\n",
    "print('Liberal sources, 1 partisan left, 2 skew left')\n",
    "response_vice1_d = response_vice1.content.decode(\"utf-8\")\n",
    "#print(response_vice1_d)\n",
    "response_vice2_d = response_vice2.content.decode(\"utf-8\")\n",
    "#print(response_vice2_d)\n",
    "\n",
    "response_msnbc1_d = response_msnbc1.content.decode(\"utf-8\")\n",
    "#print(response_msnbc1_d)\n",
    "response_msnbc2_d = response_msnbc2.content.decode(\"utf-8\")\n",
    "#print(response_msnbc2_d)\n",
    "response_msnbc3_d = response_msnbc3.content.decode(\"utf-8\")\n",
    "#print(response_msnbc3_d)\n",
    "\n",
    "response_buzz1_d = response_buzz1.content.decode(\"utf-8\")\n",
    "#print(response_buzz1_d)\n",
    "response_buzz2_d = response_buzz2.content.decode(\"utf-8\")\n",
    "#print(response_buzz2_d)\n",
    "response_buzz3_d = response_buzz3.content.decode(\"utf-8\")\n",
    "#print(response_buzz3_d)\n",
    "\n",
    "\n",
    "\n",
    "# response_hill1\n",
    "# response_hill2\n",
    "# response_reuters1\n",
    "# response_reuters2\n",
    "# response_cbs1\n",
    "# response_cbs2\n",
    "\n",
    "print('Neutral sources')\n",
    "response_hill1_d = response_hill1.content.decode(\"utf-8\")\n",
    "#print(response_hill1_d)\n",
    "response_hill2_d = response_hill2.content.decode(\"utf-8\")\n",
    "#print(response_hill2_d)\n",
    "\n",
    "response_reuters1_d = response_reuters1.content.decode(\"utf-8\")\n",
    "#print(response_reuters1_d)\n",
    "response_reuters2_d = response_reuters2.content.decode(\"utf-8\")\n",
    "#print(response_reuters2_d)\n",
    "\n",
    "response_cbs1_d = response_cbs1.content.decode(\"utf-8\")\n",
    "#print(response_cbs1_d)\n",
    "response_cbs2_d = response_cbs2.content.decode(\"utf-8\")\n",
    "#print(response_cbs2_d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(type(response_cbs2))\n",
    "type(response_cbs2_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTqEDn8XqMQs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# response_1x = json.loads(response_1)\n",
    "# print(type(response_1x))\n",
    "# # print(response_1x)\n",
    "# response_1x['articles']\n",
    "\n",
    "\n",
    "\n",
    "#conservative\n",
    "response_fox1_j = json.loads(response_fox1_d)\n",
    "# response_fox1_j['articles']\n",
    "# response_fox1_j\n",
    "response_fox2_j = json.loads(response_fox2_d)\n",
    "\n",
    "\n",
    "response_bart1_j = json.loads(response_bart1_d)\n",
    "response_bart2_j = json.loads(response_bart2_d)\n",
    "\n",
    "response_nreview1_j = json.loads(response_nreview1_d)\n",
    "response_nreview2_j = json.loads(response_nreview2_d)\n",
    "response_nreview3_j = json.loads(response_nreview3_d)\n",
    "\n",
    "#liberal\n",
    "response_vice1_j = json.loads(response_vice1_d)\n",
    "response_vice2_j = json.loads(response_vice2_d)\n",
    "\n",
    "response_msnbc1_j = json.loads(response_msnbc1_d)\n",
    "response_msnbc2_j = json.loads(response_msnbc2_d)\n",
    "response_msnbc3_j = json.loads(response_msnbc3_d)\n",
    "\n",
    "response_buzz1_j = json.loads(response_buzz1_d)\n",
    "response_buzz2_j = json.loads(response_buzz2_d)\n",
    "response_buzz3_j = json.loads(response_buzz3_d)\n",
    "\n",
    "\n",
    "#neutral\n",
    "response_hill1_j = json.loads(response_hill1_d)\n",
    "response_hill2_j = json.loads(response_hill2_d)\n",
    "\n",
    "response_reuters1_j = json.loads(response_reuters1_d)\n",
    "response_reuters2_j = json.loads(response_reuters2_d)\n",
    "\n",
    "response_cbs1_j = json.loads(response_cbs1_d)\n",
    "response_cbs2_j = json.loads(response_cbs2_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjT74B06hv_K"
   },
   "source": [
    "# Converting JSON to pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ox0m72hmeeTn"
   },
   "source": [
    "We then used pandas to transform the each JSON object into a dataframe that could be easily read and wrangled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_CbPYqQqMcE"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(response_1x['articles'])\n",
    "# df\n",
    "\n",
    "#conservative df's\n",
    "df_fox1 = pd.DataFrame(response_fox1_j['articles'])\n",
    "df_fox1\n",
    "df_fox2 = pd.DataFrame(response_fox2_j['articles'])\n",
    "df_fox2\n",
    "\n",
    "\n",
    "df_bart1 = pd.DataFrame(response_bart1_j['articles'])\n",
    "df_bart1\n",
    "df_bart2 = pd.DataFrame(response_bart2_j['articles'])\n",
    "df_bart2\n",
    " \n",
    "\n",
    "df_nreview1 = pd.DataFrame(response_nreview1_j['articles'])\n",
    "df_nreview1\n",
    "df_nreview2 = pd.DataFrame(response_nreview2_j['articles'])\n",
    "df_nreview2\n",
    "df_nreview3 = pd.DataFrame(response_nreview3_j['articles'])\n",
    "df_nreview3\n",
    "\n",
    "\n",
    "# #liberal\n",
    "\n",
    "df_vice1 = pd.DataFrame(response_vice1_j['articles'])\n",
    "df_vice1\n",
    "df_vice2 = pd.DataFrame(response_vice2_j['articles'])\n",
    "df_vice2\n",
    "\n",
    "df_msnbc1 = pd.DataFrame(response_msnbc1_j['articles'])\n",
    "df_msnbc1\n",
    "df_msnbc2 = pd.DataFrame(response_msnbc2_j['articles'])\n",
    "df_msnbc2\n",
    "df_msnbc3 = pd.DataFrame(response_msnbc3_j['articles'])\n",
    "df_msnbc3\n",
    "\n",
    "df_buzz1 = pd.DataFrame(response_buzz1_j['articles'])\n",
    "df_buzz1\n",
    "df_buzz2 = pd.DataFrame(response_buzz2_j['articles'])\n",
    "df_buzz2\n",
    "df_buzz3 = pd.DataFrame(response_buzz3_j['articles'])\n",
    "df_buzz3\n",
    "\n",
    "\n",
    "# #neutral\n",
    "df_hill1 = pd.DataFrame(response_hill1_j['articles'])\n",
    "df_hill1\n",
    "df_hill2 = pd.DataFrame(response_hill2_j['articles'])\n",
    "df_hill2\n",
    "\n",
    "df_reuters1 = pd.DataFrame(response_reuters1_j['articles'])\n",
    "df_reuters1\n",
    "df_reuters2 = pd.DataFrame(response_reuters2_j['articles'])\n",
    "df_reuters2\n",
    "\n",
    "df_cbs1 = pd.DataFrame(response_cbs1_j['articles'])\n",
    "df_cbs1\n",
    "df_cbs2 = pd.DataFrame(response_cbs1_j['articles'])\n",
    "df_cbs2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47F6gNL1j3Pk"
   },
   "source": [
    "# Merging Dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXynSQdXe0gC"
   },
   "source": [
    "After converting each JSON object into it's own dataframe, the pandas concat function was used to attach all of the rows of each of these dataframes together, creating one large dataframe of all of the scraped articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zu_vrHCFkfSX"
   },
   "outputs": [],
   "source": [
    "df_allsources = pd.concat([df_fox1, df_fox2, df_bart1, df_bart2, df_nreview1, df_nreview2, df_nreview3,\n",
    "                           df_msnbc1, df_msnbc2, df_msnbc3, df_buzz1, df_buzz2, df_buzz3, df_vice1, df_vice2,\n",
    "                           df_hill1, df_hill2, df_reuters1, df_reuters2, df_cbs1, df_cbs2])\n",
    "df_allsources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuJA91HYl8zT"
   },
   "outputs": [],
   "source": [
    "df_allsources.to_csv('newsapi_scrape1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEt8SbNaaj_"
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PaXPnO0aakA"
   },
   "source": [
    "We first had a conversation about which part of the API's we were going to be using. There were news articles that didn't have content, and if it did have content, it wasn't fully there. So going forward with cleaning we decided that we should gather all that we could as far as content wise, and we could decide what to do with it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OE8lGfyXaakB",
    "outputId": "04d199c2-3164-4824-f8bc-573bfd7e04cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 453 news articles that do not have content\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "MarchNews = pd.read_csv('newsapi_scrape1.csv')\n",
    "#print(MarchNews)\n",
    "\n",
    "Nan = MarchNews['content'].isnull().sum()\n",
    "print(f'There are {Nan} news articles that do not have content')\n",
    "\n",
    "\n",
    "#Replacing Null values with 'None'\n",
    "MarchNews['content'].fillna(\"None\", inplace = True)\n",
    "MarchNews['title'].fillna('None', inplace = True)\n",
    "MarchNews['description'].fillna('None', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbOwDRKsaakF"
   },
   "source": [
    "The code underneath is finding all of the distinct words in the content, description, and title columns. Also using the re.findall function we were also able to remove the punctuation from the columns as well. After combing through the data we realized that the last two words were for the number of characters after whatever we had, so we scraped the last two entries in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfZpT6lraakG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "Content = []\n",
    "Description = []\n",
    "Title = []\n",
    "\n",
    "for i in range(len(MarchNews['content'])):\n",
    "    Content.append(re.findall(r'\\w+', MarchNews['content'][i])[:-3])\n",
    "    Description.append(re.findall(r'\\w+', MarchNews['description'][i]))\n",
    "    Title.append(re.findall(r'\\w+', MarchNews['title'][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dvl5q-P6aakK"
   },
   "source": [
    "The codeblock below is taking out any stopwords, and removing any numbers or unknown symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmwQUPGqaakK"
   },
   "outputs": [],
   "source": [
    "# Getting rid of Stopwords\n",
    "\n",
    "for i in range(len(Content)):\n",
    "    # Taking out the Stopwords\n",
    "    Content[i] = [words.lower() for words in Content[i] if words.lower() not in stopwords]\n",
    "    Title[i] = [words.lower() for words in Title[i] if words.lower() not in stopwords]\n",
    "    Description[i] = [words.lower() for words in Description[i] if words.lower() not in stopwords]\n",
    "    \n",
    "    # Taking out the non-string values\n",
    "    Content[i] = [words for words in Content[i] if words.isalpha() == True]\n",
    "    Title[i] = [words for words in Title[i] if words.isalpha() == True]\n",
    "    Description[i] = [words for words in Description[i] if words.isalpha() == True]\n",
    "    \n",
    "#print(Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgpOdbTQaakN"
   },
   "outputs": [],
   "source": [
    "MarchNews.insert(8,'TitleCleaned', Title)\n",
    "MarchNews.insert(9,'DescriptionCleaned', Description)\n",
    "MarchNews.insert(10,'ContentCleaned', Content)\n",
    "#print(MarchNews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "mwWQNk6maakP",
    "outputId": "168d7f93-e248-4a7c-cad4-584b7b59b038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1784 entries, 0 to 1783\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   source              1784 non-null   object\n",
      " 1   author              1745 non-null   object\n",
      " 2   title               1784 non-null   object\n",
      " 3   description         1784 non-null   object\n",
      " 4   url                 1784 non-null   object\n",
      " 5   urlToImage          1751 non-null   object\n",
      " 6   publishedAt         1784 non-null   object\n",
      " 7   content             1784 non-null   object\n",
      " 8   TitleCleaned        1784 non-null   object\n",
      " 9   DescriptionCleaned  1784 non-null   object\n",
      " 10  ContentCleaned      1784 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 153.4+ KB\n"
     ]
    }
   ],
   "source": [
    "MarchNews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OP7IiPEWT_U7"
   },
   "source": [
    "## Produce new Columns\n",
    "Create a column of unique words in each article and a column of all words in article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fi4czRyHT_U8",
    "outputId": "605dcdef-4b8c-428f-e461-f051906d2a6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>TitleCleaned</th>\n",
       "      <th>DescriptionCleaned</th>\n",
       "      <th>ContentCleaned</th>\n",
       "      <th>uniqueWords</th>\n",
       "      <th>allWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'fox-news', 'name': 'Fox News'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sanders supporters react to DNC reportedly att...</td>\n",
       "      <td>Fox News contributor Lawrence Jones speaks to ...</td>\n",
       "      <td>http://video.foxnews.com/v/6136724620001/</td>\n",
       "      <td>https://cf-images.us-east-1.prod.boltdns.net/v...</td>\n",
       "      <td>2020-02-28T03:43:40Z</td>\n",
       "      <td>None</td>\n",
       "      <td>[sanders, supporters, react, dnc, reportedly, ...</td>\n",
       "      <td>[fox, news, contributor, lawrence, jones, spea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{jones, primary, give, reportedly, speaks, vot...</td>\n",
       "      <td>[sanders, supporters, react, dnc, reportedly, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source author  \\\n",
       "1  {'id': 'fox-news', 'name': 'Fox News'}    NaN   \n",
       "\n",
       "                                               title  \\\n",
       "1  Sanders supporters react to DNC reportedly att...   \n",
       "\n",
       "                                         description  \\\n",
       "1  Fox News contributor Lawrence Jones speaks to ...   \n",
       "\n",
       "                                         url  \\\n",
       "1  http://video.foxnews.com/v/6136724620001/   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "1  https://cf-images.us-east-1.prod.boltdns.net/v...  2020-02-28T03:43:40Z   \n",
       "\n",
       "  content                                       TitleCleaned  \\\n",
       "1    None  [sanders, supporters, react, dnc, reportedly, ...   \n",
       "\n",
       "                                  DescriptionCleaned ContentCleaned  \\\n",
       "1  [fox, news, contributor, lawrence, jones, spea...             []   \n",
       "\n",
       "                                         uniqueWords  \\\n",
       "1  {jones, primary, give, reportedly, speaks, vot...   \n",
       "\n",
       "                                            allWords  \n",
       "1  [sanders, supporters, react, dnc, reportedly, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words for each column\n",
    "uniqueWords = []\n",
    "allWords = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    # create empty lsit each iteration\n",
    "    lst = []\n",
    "    # get all word in title, content, and description\n",
    "    [lst.append(words) for words in MarchNews['TitleCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['ContentCleaned'][i]]\n",
    "    [lst.append(words) for words in MarchNews['DescriptionCleaned'][i]]\n",
    "    # assign all words list to each row of whole df\n",
    "    allWords.append(lst)\n",
    "    # transform list into set of unique words\n",
    "    unique = set(lst)\n",
    "    #assign unique word set to each row of whole df\n",
    "    uniqueWords.append(unique)\n",
    "    \n",
    "# create new row in big DF with unique words\n",
    "MarchNews.insert(11,'uniqueWords', uniqueWords)\n",
    "MarchNews.insert(12, 'allWords', allWords)\n",
    "MarchNews[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6wVKGg0T_VA",
    "outputId": "17365e9a-17d6-407f-8fc8-eaa54bf3f143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "{'primary', 'primaries', 'news', 'away', 'mollie', 'biden', 'win', 'carolina', 'sunday', 'former', 'democratic', 'fox', 'carry', 'blew', 'called', 'south', 'people', 'appeared', 'victory', 'confident', 'comeback', 'sc', 'saturday', 'upcoming', 'federalist', 'editor', 'super', 'night', 'expectations', 'hemingway', 'tuesday', 'looks', 'real', 'president', 'joe', 'momentum', 'seemed', 'vice'}\n"
     ]
    }
   ],
   "source": [
    "print(len(MarchNews['uniqueWords'][3]))\n",
    "print(MarchNews['uniqueWords'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "['federalist', 'editor', 'mollie', 'hemingway', 'biden', 'sc', 'win', 'blew', 'away', 'people', 'expectations', 'former', 'vice', 'president', 'joe', 'biden', 'called', 'victory', 'saturday', 'night', 'south', 'carolina', 'democratic', 'primary', 'real', 'comeback', 'looks', 'carry', 'momentum', 'upcoming', 'super', 'tuesday', 'primaries', 'biden', 'appeared', 'fox', 'news', 'sunday', 'seemed', 'confident', 'former', 'vice', 'president', 'joe', 'biden', 'called', 'victory', 'saturday', 'night', 'south', 'carolina', 'democratic', 'primary', 'real', 'comeback', 'looks', 'carry', 'momentum', 'upcoming', 'super', 'tuesday', 'primaries', 'biden', 'appeared', 'fox', 'news', 'sunday', 'seemed', 'confident']\n"
     ]
    }
   ],
   "source": [
    "print(len(MarchNews['allWords'][3]))\n",
    "print(MarchNews['allWords'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column of all words in an article saved as a single string. This will be fed into the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>TitleCleaned</th>\n",
       "      <th>DescriptionCleaned</th>\n",
       "      <th>ContentCleaned</th>\n",
       "      <th>uniqueWords</th>\n",
       "      <th>allWords</th>\n",
       "      <th>allWords_sentenceString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'fox-news', 'name': 'Fox News'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sanders supporters react to DNC reportedly att...</td>\n",
       "      <td>Fox News contributor Lawrence Jones speaks to ...</td>\n",
       "      <td>http://video.foxnews.com/v/6136724620001/</td>\n",
       "      <td>https://cf-images.us-east-1.prod.boltdns.net/v...</td>\n",
       "      <td>2020-02-28T03:43:40Z</td>\n",
       "      <td>None</td>\n",
       "      <td>[sanders, supporters, react, dnc, reportedly, ...</td>\n",
       "      <td>[fox, news, contributor, lawrence, jones, spea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{jones, primary, give, reportedly, speaks, vot...</td>\n",
       "      <td>[sanders, supporters, react, dnc, reportedly, ...</td>\n",
       "      <td>sanders supporters react dnc reportedly attemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'fox-news', 'name': 'Fox News'}</td>\n",
       "      <td>William Sanchez</td>\n",
       "      <td>Contentious Race Democratic Race Heads to Sout...</td>\n",
       "      <td>We are days away from the South Carolina prima...</td>\n",
       "      <td>https://radio.foxnews.com/2020/02/27/the-fox-n...</td>\n",
       "      <td>https://radio.foxnews.com/wp-content/uploads/2...</td>\n",
       "      <td>2020-02-27T10:00:02Z</td>\n",
       "      <td>We are days away from the South Carolina prima...</td>\n",
       "      <td>[contentious, race, democratic, race, heads, s...</td>\n",
       "      <td>[days, away, south, carolina, primary, last, p...</td>\n",
       "      <td>[days, away, south, carolina, primary, last, p...</td>\n",
       "      <td>{reed, primary, chief, heads, contentious, dis...</td>\n",
       "      <td>[contentious, race, democratic, race, heads, s...</td>\n",
       "      <td>contentious race democratic race heads south c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source           author  \\\n",
       "1  {'id': 'fox-news', 'name': 'Fox News'}              NaN   \n",
       "2  {'id': 'fox-news', 'name': 'Fox News'}  William Sanchez   \n",
       "\n",
       "                                               title  \\\n",
       "1  Sanders supporters react to DNC reportedly att...   \n",
       "2  Contentious Race Democratic Race Heads to Sout...   \n",
       "\n",
       "                                         description  \\\n",
       "1  Fox News contributor Lawrence Jones speaks to ...   \n",
       "2  We are days away from the South Carolina prima...   \n",
       "\n",
       "                                                 url  \\\n",
       "1          http://video.foxnews.com/v/6136724620001/   \n",
       "2  https://radio.foxnews.com/2020/02/27/the-fox-n...   \n",
       "\n",
       "                                          urlToImage           publishedAt  \\\n",
       "1  https://cf-images.us-east-1.prod.boltdns.net/v...  2020-02-28T03:43:40Z   \n",
       "2  https://radio.foxnews.com/wp-content/uploads/2...  2020-02-27T10:00:02Z   \n",
       "\n",
       "                                             content  \\\n",
       "1                                               None   \n",
       "2  We are days away from the South Carolina prima...   \n",
       "\n",
       "                                        TitleCleaned  \\\n",
       "1  [sanders, supporters, react, dnc, reportedly, ...   \n",
       "2  [contentious, race, democratic, race, heads, s...   \n",
       "\n",
       "                                  DescriptionCleaned  \\\n",
       "1  [fox, news, contributor, lawrence, jones, spea...   \n",
       "2  [days, away, south, carolina, primary, last, p...   \n",
       "\n",
       "                                      ContentCleaned  \\\n",
       "1                                                 []   \n",
       "2  [days, away, south, carolina, primary, last, p...   \n",
       "\n",
       "                                         uniqueWords  \\\n",
       "1  {jones, primary, give, reportedly, speaks, vot...   \n",
       "2  {reed, primary, chief, heads, contentious, dis...   \n",
       "\n",
       "                                            allWords  \\\n",
       "1  [sanders, supporters, react, dnc, reportedly, ...   \n",
       "2  [contentious, race, democratic, race, heads, s...   \n",
       "\n",
       "                             allWords_sentenceString  \n",
       "1  sanders supporters react dnc reportedly attemp...  \n",
       "2  contentious race democratic race heads south c...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set allWords as a single string\n",
    "allWords_string = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    word_list = MarchNews['allWords'][i]\n",
    "    word_string =  (' '.join(word for word in word_list))\n",
    "    allWords_string.append(word_string)\n",
    "    \n",
    "MarchNews.insert(13, 'allWords_sentenceString', allWords_string)\n",
    "MarchNews[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bias column. To be used as the target in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with political leaning\n",
    "\n",
    "# get source names and store in array\n",
    "source_names = MarchNews.source.unique()\n",
    "\n",
    "# create df with political leaning assigned to each source (-1,0,1 == left, center, right)\n",
    "source_bias = {'source':source_names, 'bias':[1,1,1,-1,-1,-1,0,0,0]}\n",
    "source_bias_df = pd.DataFrame(data=source_bias)\n",
    "#source_bias_df\n",
    "\n",
    "# add bias column to main df\n",
    "MarchNews = MarchNews.merge(source_bias_df, on='source', how='left')\n",
    "#MarchNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DF with just left and right souces. \n",
    "To be fed into binary ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get df with just left and right sorces for binary classifiers\n",
    "mask = MarchNews[\"bias\"] !=0\n",
    "MarchNews_noMiddle = MarchNews.loc[mask]\n",
    "MarchNews_noMiddle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word count in matrix \n",
    "This is done for both the whole dataset, and the df with no center sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZzkAU7dT_VD",
    "outputId": "e97f751c-d585-4dec-b75e-c81ec8ab248b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9287\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "# get unique words for whole dataset\n",
    "uniqueWords_all = []\n",
    "\n",
    "for i in range(len(MarchNews['ContentCleaned'])):\n",
    "    # get all words in title, content, and description\n",
    "    [uniqueWords_all.append(words) for words in MarchNews['allWords'][i]]\n",
    "uniqueWords_all = set(uniqueWords_all)\n",
    "print(len(uniqueWords_all))\n",
    "\n",
    "# get unique words dataset without centrist sources\n",
    "uniqueWords_all_noMiddle = []\n",
    "\n",
    "for i in range(len(MarchNews_noMiddle['ContentCleaned'])):\n",
    "    # get all words in title, content, and description\n",
    "    [uniqueWords_all_noMiddle.append(words) for words in MarchNews['allWords'][i]]\n",
    "uniqueWords_all_noMiddle = set(uniqueWords_all_noMiddle)\n",
    "print(len(uniqueWords_all_noMiddle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxWX0_CHT_VH",
    "outputId": "c2fb59ac-ce5a-4747-e5ab-b9eb4022655d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimi</th>\n",
       "      <th>advisory</th>\n",
       "      <th>nyt</th>\n",
       "      <th>brooks</th>\n",
       "      <th>hill</th>\n",
       "      <th>mutton</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>ultrasound</th>\n",
       "      <th>improve</th>\n",
       "      <th>zeroes</th>\n",
       "      <th>...</th>\n",
       "      <th>apologizes</th>\n",
       "      <th>helene</th>\n",
       "      <th>multimillion</th>\n",
       "      <th>ly</th>\n",
       "      <th>mornings</th>\n",
       "      <th>cities</th>\n",
       "      <th>stabbing</th>\n",
       "      <th>becoming</th>\n",
       "      <th>crossings</th>\n",
       "      <th>banks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 9287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mimi, advisory, nyt, brooks, hill, mutton, exchanges, ultrasound, improve, zeroes, millennials, historyfox, revealing, spotmeet, tarlov, chatting, typically, theatres, bolster, largely, regulators, hospitalization, credit, conviction, cortezthe, union, pub, deleted, overwhelming, presidency, careless, wondered, ariz, nafta, mode, celebrities, history, actress, subpoena, fled, popularizedand, jurisdiction, lego, claire, enfrentar, rouhani, muriel, floating, 自民党の山田宏参議院議員も, angelo, britain, lockdowns, freezes, analyst, respirator, held, payout, prevented, mid, joe, teaching, glimpse, hopes, turnout, fury, speak, embedin, pollkarl, exi, shakir, dad, intention, awarding, alight, extended, wfh, connecting, sunak, kicked, understanding, crashing, esquerda, driver, headline, playboy, spirit, cheese, retailers, hiv, relaxante, indication, sza, real, casa, summoned, killing, airline, yuji, canadian, copy, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 9287 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df with unique words as col names (over 9000 cols)\n",
    "wordCountMatrix = pd.DataFrame(index=[], columns=uniqueWords_all)\n",
    "wordCountMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimi</th>\n",
       "      <th>advisory</th>\n",
       "      <th>nyt</th>\n",
       "      <th>brooks</th>\n",
       "      <th>hill</th>\n",
       "      <th>mutton</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>ultrasound</th>\n",
       "      <th>improve</th>\n",
       "      <th>revealing</th>\n",
       "      <th>...</th>\n",
       "      <th>tropical</th>\n",
       "      <th>google</th>\n",
       "      <th>brought</th>\n",
       "      <th>secures</th>\n",
       "      <th>system</th>\n",
       "      <th>apologizes</th>\n",
       "      <th>multimillion</th>\n",
       "      <th>cities</th>\n",
       "      <th>stabbing</th>\n",
       "      <th>becoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 7500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mimi, advisory, nyt, brooks, hill, mutton, exchanges, ultrasound, improve, revealing, tarlov, chatting, typically, theatres, bolster, hospitalization, conviction, union, deleted, presidency, careless, wondered, ariz, nafta, mode, celebrities, history, actress, fled, popularizedand, jurisdiction, claire, enfrentar, rouhani, floating, 自民党の山田宏参議院議員も, angelo, britain, lockdowns, freezes, analyst, held, payout, prevented, mid, joe, hopes, turnout, fury, speak, embedin, shakir, dad, alight, extended, wfh, connecting, sunak, kicked, understanding, crashing, esquerda, driver, headline, playboy, spirit, cheese, retailers, hiv, relaxante, indication, sza, real, casa, summoned, killing, airline, canadian, copy, meanwhile, thousands, testing, lunch, amongst, fold, contaminated, units, norovirus, counselor, predicts, pronounce, showed, shrinks, identify, making, success, solace, tuesday, flags, monitored, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 7500 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty df with unique words as col names (over 7500 cols)\n",
    "wordCountMatrix_noMiddle = pd.DataFrame(index=[], columns=uniqueWords_all_noMiddle)\n",
    "wordCountMatrix_noMiddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAPVu_s9T_VL"
   },
   "outputs": [],
   "source": [
    "# Fill in wordCountMatrix (count of each word within each article)\n",
    "\n",
    "# iterate through whole dataset\n",
    "for row in range(len(MarchNews['ContentCleaned'])):\n",
    "    \n",
    "    # initialize empty count list\n",
    "    count_lst = []\n",
    "\n",
    "    # set unique words as column names\n",
    "    cols = MarchNews['uniqueWords'][row]\n",
    "\n",
    "    # iterate through column names\n",
    "    for i in cols:\n",
    "        # start count at 0, add counted words from each section \n",
    "        counter = 0\n",
    "        counter += MarchNews['allWords'][row].count(i)\n",
    "\n",
    "        # add final count of words to list\n",
    "        count_lst.append(counter)\n",
    "\n",
    "    # create df with this article'ss unique words as col names, and counts as a row\n",
    "    count_row_temp_df = pd.DataFrame([count_lst], columns=cols)\n",
    "\n",
    "    # append this df/row to wordCountMatrix \n",
    "    wordCountMatrix = wordCountMatrix.append(count_row_temp_df, ignore_index=True, sort=False)\n",
    "\n",
    "# replace NaNs with 0\n",
    "wordCountMatrix = wordCountMatrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAg3ge1oT_VS",
    "outputId": "7ed710ae-41c1-4167-8556-595608232a17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimi</th>\n",
       "      <th>advisory</th>\n",
       "      <th>nyt</th>\n",
       "      <th>brooks</th>\n",
       "      <th>hill</th>\n",
       "      <th>mutton</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>ultrasound</th>\n",
       "      <th>improve</th>\n",
       "      <th>zeroes</th>\n",
       "      <th>...</th>\n",
       "      <th>apologizes</th>\n",
       "      <th>helene</th>\n",
       "      <th>multimillion</th>\n",
       "      <th>ly</th>\n",
       "      <th>mornings</th>\n",
       "      <th>cities</th>\n",
       "      <th>stabbing</th>\n",
       "      <th>becoming</th>\n",
       "      <th>crossings</th>\n",
       "      <th>banks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1784 rows × 9287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mimi  advisory  nyt  brooks  hill  mutton  exchanges  ultrasound  \\\n",
       "0        0         0    0       0     0       0          0           0   \n",
       "1        0         0    0       0     0       0          0           0   \n",
       "2        0         0    0       0     0       0          0           0   \n",
       "3        0         0    0       0     0       0          0           0   \n",
       "4        0         0    0       0     0       0          0           0   \n",
       "...    ...       ...  ...     ...   ...     ...        ...         ...   \n",
       "1779     0         0    0       0     0       0          0           0   \n",
       "1780     0         0    0       0     0       0          0           0   \n",
       "1781     0         0    0       0     0       0          0           0   \n",
       "1782     0         0    0       0     0       0          0           0   \n",
       "1783     0         0    0       0     0       0          0           0   \n",
       "\n",
       "      improve  zeroes  ...  apologizes  helene  multimillion  ly  mornings  \\\n",
       "0           0       0  ...           0       0             0   0         0   \n",
       "1           0       0  ...           0       0             0   0         0   \n",
       "2           0       0  ...           0       0             0   0         0   \n",
       "3           0       0  ...           0       0             0   0         0   \n",
       "4           0       0  ...           0       0             0   0         0   \n",
       "...       ...     ...  ...         ...     ...           ...  ..       ...   \n",
       "1779        0       0  ...           0       0             0   0         0   \n",
       "1780        0       0  ...           0       0             0   0         0   \n",
       "1781        0       0  ...           0       0             0   0         0   \n",
       "1782        0       0  ...           0       0             0   0         0   \n",
       "1783        0       0  ...           0       0             0   0         0   \n",
       "\n",
       "      cities  stabbing  becoming  crossings  banks  \n",
       "0          0         0         0          0      0  \n",
       "1          0         0         0          0      0  \n",
       "2          0         0         0          0      0  \n",
       "3          0         0         0          0      0  \n",
       "4          0         0         0          0      0  \n",
       "...      ...       ...       ...        ...    ...  \n",
       "1779       0         0         0          0      0  \n",
       "1780       0         0         0          0      0  \n",
       "1781       0         0         0          0      0  \n",
       "1782       0         0         0          0      0  \n",
       "1783       0         0         0          0      0  \n",
       "\n",
       "[1784 rows x 9287 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVb6N8C6T_VW",
    "outputId": "ad81a0b0-838c-40a9-dfb6-50132b5fe790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       5\n",
       "4       2\n",
       "       ..\n",
       "1779    0\n",
       "1780    0\n",
       "1781    0\n",
       "1782    0\n",
       "1783    0\n",
       "Name: biden, Length: 1784, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountMatrix['biden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but for the _noMiddle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in wordCountMatrix_noMiddle (count of each word within each article)\n",
    "\n",
    "# iterate through whole dataset\n",
    "for row in range(len(MarchNews_noMiddle['ContentCleaned'])):\n",
    "    \n",
    "    # initialize empty count list\n",
    "    count_lst = []\n",
    "\n",
    "    # set unique words as column names\n",
    "    cols = MarchNews_noMiddle['uniqueWords'][row]\n",
    "\n",
    "    # iterate through column names\n",
    "    for i in cols:\n",
    "        # start count at 0, add counted words from each section \n",
    "        counter = 0\n",
    "        counter += MarchNews_noMiddle['allWords'][row].count(i)\n",
    "\n",
    "        # add final count of words to list\n",
    "        count_lst.append(counter)\n",
    "\n",
    "    # create df with this article's unique words as col names, and counts as a row\n",
    "    count_row_temp_df = pd.DataFrame([count_lst], columns=cols)\n",
    "\n",
    "    # append this df/row to wordCountMatrix \n",
    "    wordCountMatrix_noMiddle = wordCountMatrix_noMiddle.append(count_row_temp_df, ignore_index=True, sort=False)\n",
    "\n",
    "# replace NaNs with 0\n",
    "wordCountMatrix_noMiddle = wordCountMatrix_noMiddle.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimi</th>\n",
       "      <th>advisory</th>\n",
       "      <th>nyt</th>\n",
       "      <th>brooks</th>\n",
       "      <th>hill</th>\n",
       "      <th>mutton</th>\n",
       "      <th>exchanges</th>\n",
       "      <th>ultrasound</th>\n",
       "      <th>improve</th>\n",
       "      <th>revealing</th>\n",
       "      <th>...</th>\n",
       "      <th>tropical</th>\n",
       "      <th>google</th>\n",
       "      <th>brought</th>\n",
       "      <th>secures</th>\n",
       "      <th>system</th>\n",
       "      <th>apologizes</th>\n",
       "      <th>multimillion</th>\n",
       "      <th>cities</th>\n",
       "      <th>stabbing</th>\n",
       "      <th>becoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1184 rows × 7500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mimi  advisory  nyt  brooks  hill  mutton  exchanges  ultrasound  \\\n",
       "0        0         0    0       0     0       0          0           0   \n",
       "1        0         0    0       0     0       0          0           0   \n",
       "2        0         0    0       0     0       0          0           0   \n",
       "3        0         0    0       0     0       0          0           0   \n",
       "4        0         0    0       0     0       0          0           0   \n",
       "...    ...       ...  ...     ...   ...     ...        ...         ...   \n",
       "1179     0         0    0       0     0       0          0           0   \n",
       "1180     0         0    0       0     0       0          0           0   \n",
       "1181     0         0    0       0     0       0          0           0   \n",
       "1182     0         0    0       0     0       0          0           0   \n",
       "1183     0         0    0       0     0       0          0           0   \n",
       "\n",
       "      improve  revealing  ...  tropical  google  brought  secures  system  \\\n",
       "0           0          0  ...         0       0        0        0       0   \n",
       "1           0          0  ...         0       0        0        0       0   \n",
       "2           0          0  ...         0       0        0        0       0   \n",
       "3           0          0  ...         0       0        0        0       0   \n",
       "4           0          0  ...         0       0        0        0       0   \n",
       "...       ...        ...  ...       ...     ...      ...      ...     ...   \n",
       "1179        0          0  ...         0       0        0        0       0   \n",
       "1180        0          0  ...         0       0        0        0       0   \n",
       "1181        0          0  ...         0       0        0        0       0   \n",
       "1182        0          0  ...         0       0        0        0       0   \n",
       "1183        0          0  ...         0       0        0        0       0   \n",
       "\n",
       "      apologizes  multimillion  cities  stabbing  becoming  \n",
       "0              0             0       0         0         0  \n",
       "1              0             0       0         0         0  \n",
       "2              0             0       0         0         0  \n",
       "3              0             0       0         0         0  \n",
       "4              0             0       0         0         0  \n",
       "...          ...           ...     ...       ...       ...  \n",
       "1179           0             0       0         0         0  \n",
       "1180           0             0       0         0         0  \n",
       "1181           0             0       0         1         0  \n",
       "1182           0             0       0         0         0  \n",
       "1183           0             0       0         0         0  \n",
       "\n",
       "[1184 rows x 7500 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountMatrix_noMiddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML methods: Binary Classifiers(Perceptron and Logistic Reg) and Multi-Classifiers (SVM, Bayes) \n",
    "Split into training and test data for Binary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binary vectors\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get data and targets\n",
    "X = MarchNews_noMiddle['allWords_sentenceString'].tolist() #all words/data\n",
    "y = np.asarray(MarchNews_noMiddle['bias'])  #bias/target\n",
    "\n",
    "# vectorizer (TfidfVectorizer, CountVectorizer, and build by us)\n",
    "vectorizer_TF = TfidfVectorizer(min_df = 2, max_df = .6)\n",
    "TFVectorizer_X = vectorizer_TF.fit_transform(X)\n",
    "\n",
    "vectorizer_CV = CountVectorizer(min_df = 2, max_df = .6)\n",
    "CVectorizer_X = vectorizer_CV.fit_transform(X)\n",
    "\n",
    "# HomeGrown_X = wordCountMatrix\n",
    "\n",
    "# split into train and test data\n",
    "Xtrain_TF, Xtest_TF, ytrain, ytest = train_test_split(TFVectorizer_X, y, random_state=1, test_size=0.8)\n",
    "Xtrain_CV, Xtest_CV, ytrain, ytest = train_test_split(CVectorizer_X, y, random_state=1, test_size=0.8)\n",
    "Xtrain_HomeGrown, Xtest_HomeGrown, ytrain, ytest = train_test_split(wordCountMatrix_noMiddle, y, random_state=1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using  TfidfVectorizer= 0.6592827004219409\n",
      "Accuracy using  CountVectorizer= 0.7805907172995781\n",
      "Accuracy using  CountVectorizer= 0.7805907172995781\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Using TfidfVectorizer\n",
    "LR_TF = LogisticRegression(random_state=0).fit(Xtrain_TF, ytrain)\n",
    "y_TF = LR_TF.predict(Xtest_TF)\n",
    "print('Accuracy using  TfidfVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_TF))\n",
    "\n",
    "# Using CountVectorizer\n",
    "LR_CV = LogisticRegression(random_state=0).fit(Xtrain_CV, ytrain)\n",
    "y_CV = LR_TF.predict(Xtest_CV)\n",
    "print('Accuracy using  CountVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_CV))\n",
    "\n",
    "# Using HomeGrown Matrix\n",
    "LR_HG = LogisticRegression(random_state=0).fit(Xtrain_HomeGrown, ytrain)\n",
    "y_HG = LR_HG.predict(Xtest_HomeGrown)\n",
    "print('Accuracy using  CountVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using  TfidfVectorizer= 0.7373417721518988\n",
      "Accuracy using  CountVectorizer= 0.7584388185654009\n",
      "Accuracy using  CountVectorizer= 0.7584388185654009\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Using TfidfVectorizer\n",
    "MLP_TF = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1).fit(Xtrain_TF, ytrain)\n",
    "y_TF = MLP_TF.predict(Xtest_TF)\n",
    "print('Accuracy using  TfidfVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_TF))\n",
    "\n",
    "# Using CountVectorizer\n",
    "MLP_CV = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1).fit(Xtrain_CV, ytrain)\n",
    "y_CV = MLP_TF.predict(Xtest_CV)\n",
    "print('Accuracy using  CountVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_CV))\n",
    "\n",
    "# Using HomeGrown Matrix\n",
    "MLP_HG = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1).fit(Xtrain_HomeGrown, ytrain)\n",
    "y_HG = MLP_HG.predict(Xtest_HomeGrown)\n",
    "print('Accuracy using  CountVectorizer=', metrics.accuracy_score(y_true = ytest, y_pred = y_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHg8EsqKUowl"
   },
   "source": [
    "## Using CountVectorizer to get  two word, three word matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_31TlCXjT_Vi",
    "outputId": "6a225e15-d373-4be4-c9eb-56a76232cd6c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Count Vectorizer\n",
    "countvectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "vector_bi_count = countvectorizer_bigram.fit_transform(MarchNews['allWords_sentenceString'])\n",
    "print(vector_bi_count.shape)\n",
    "\n",
    "#Tfidf Vectorizer\n",
    "tfidfvectorizer_bigram = TfidfVectorizer(ngram_range=(2,2))\n",
    "vector_bi_tfidf = tfidfvectorizer_bigram.fit_transform(MarchNews['allWords_sentenceString'])\n",
    "vector_bi_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F9QzhkbRddTE",
    "outputId": "5342616d-b9ae-404c-f989-7305e6c806c0"
   },
   "outputs": [],
   "source": [
    "#Count vectorizer for trigram\n",
    "countvectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
    "vector_tri_count = countvectorizer_trigram.fit_transform(MarchNews['allWords_sentenceString'])\n",
    "print(vector_tri_count.shape)\n",
    "\n",
    "# #Tfidf vectorizer for trigram\n",
    "tfidfvectorizer_trigram = TfidfVectorizer(ngram_range=(3,3))\n",
    "vector_tri_tfidf = tfidfvectorizer_trigram.fit_transform(MarchNews['allWords_sentenceString'])\n",
    "vector_tri_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZNXsBwqGdvEh",
    "outputId": "14574972-be1b-4f30-aa66-a628ae055ac7"
   },
   "outputs": [],
   "source": [
    "# type(vector4)   #turn into viewable dataframe later?\n",
    "# vector4.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "1BLkryJxeMeD",
    "outputId": "0c6dea79-e38a-4d06-959a-32f8f139183c"
   },
   "source": [
    "# Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "yD5SSD97T_Vl",
    "outputId": "14aef787-53ed-495f-9ba1-de85c991c6d3"
   },
   "outputs": [],
   "source": [
    "y = np.asarray(MarchNews['bias'])  #bias/target\n",
    "\n",
    "#bigram count\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(vector_bi_count, y, test_size=0.8, random_state=1) \n",
    "#bigram tfidf\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(vector_bi_tfidf, y, test_size=0.8, random_state=1) \n",
    "\n",
    "#trigram count\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(vector_tri_count, y, test_size=0.8, random_state=1) \n",
    "#trigram tfidf\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(vector_tri_tfidf, y, test_size=0.8, random_state=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Count Vector with bigrams\n",
    "clf = MultinomialNB(alpha = 1)\n",
    "clf.fit(X_train1, y_train1)\n",
    "y_pred = clf.predict(X_test1)\n",
    "metrics.f1_score(y_test1, y_pred, average='macro')\n",
    "\n",
    "#.74 with binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf vector with bigrams\n",
    "clf = MultinomialNB(alpha = .1)\n",
    "clf.fit(X_train2, y_train2)\n",
    "y_pred = clf.predict(X_test2)\n",
    "metrics.f1_score(y_test2, y_pred, average='macro')\n",
    "\n",
    "#.6 with binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vector with trigrams\n",
    "clf = MultinomialNB(alpha = .5)\n",
    "clf.fit(X_train3, y_train3)\n",
    "y_pred = clf.predict(X_test3)\n",
    "metrics.f1_score(y_test3, y_pred, average='macro')\n",
    "#.66 with binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf vector with trigrams\n",
    "clf = MultinomialNB(alpha = .01)\n",
    "clf.fit(X_train4, y_train4)\n",
    "y_pred = clf.predict(X_test4)\n",
    "metrics.f1_score(y_test4, y_pred, average='macro')\n",
    "# .47 with binary target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MarchNews_noMiddle['allWords_sentenceString'].tolist() #all words/data\n",
    "y = np.asarray(MarchNews_noMiddle['bias'])  #bias/target\n",
    "\n",
    "\n",
    "\n",
    "Xtrain_TF, Xtest_TF, ytrain, ytest = train_test_split(TFVectorizer_X, y, random_state=1, test_size=0.8)\n",
    "Xtrain_CV, Xtest_CV, ytrain, ytest = train_test_split(CVectorizer_X, y, random_state=1, test_size=0.8)\n",
    "Xtrain_HomeGrown, Xtest_HomeGrown, ytrain, ytest = train_test_split(wordCountMatrix_noMiddle, y, random_state=1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = .18)\n",
    "clf.fit(Xtrain_TF, ytrain)\n",
    "y_pred = clf.predict(Xtest_TF)\n",
    "metrics.f1_score(ytest, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 1.2)\n",
    "clf.fit(Xtrain_CV, ytrain)\n",
    "y_pred = clf.predict(Xtest_CV)\n",
    "metrics.f1_score(ytest, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 1)\n",
    "clf.fit(Xtrain_HomeGrown, ytrain)\n",
    "y_pred = clf.predict(Xtest_HomeGrown)\n",
    "metrics.f1_score(ytest, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram, count vector, three targets\n",
    "\n",
    "modelsvm = svm.SVC(kernel='rbf',C=40, gamma='scale')\n",
    "modelsvm.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = modelsvm.predict(X_test1)\n",
    "print(metrics.confusion_matrix(y_true = y_test1, y_pred = y_pred))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test1, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram, tfidf vector, three targets\n",
    "\n",
    "modelsvm = svm.SVC(kernel='rbf',C=20, gamma='scale')\n",
    "modelsvm.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = modelsvm.predict(X_test2)\n",
    "print(metrics.confusion_matrix(y_true = y_test2, y_pred = y_pred))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test2, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigram binary targets\n",
    "\n",
    "modelsvm = svm.SVC(kernel='rbf',C=1.6, gamma='scale')\n",
    "modelsvm.fit(Xtrain_CV, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = modelsvm.predict(Xtest_CV)\n",
    "print(metrics.confusion_matrix(y_true = ytest, y_pred = y_pred))\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = ytest, y_pred = y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DetectingMediaBias.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
